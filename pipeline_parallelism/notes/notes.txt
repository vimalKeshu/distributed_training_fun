kubectl logs -n dev-env \
  -l training.kubeflow.org/job-name=pp-multinode-job \
  -c pytorch \
  --prefix

export NNODES=${PET_NNODES:1}
export NPROC_PER_NODE=${NPROC_PER_NODE:-8}
export MASTER_ADDR=${MASTER_ADDR:-localhost}
export MASTER_PORT=${MASTER_PORT:-23456}
export BATCH_SIZE=${BATCH_SIZE:-256}
export MICRO_BATCHES=${MICRO_BATCHES:-64}
export LEARNING_RATE=${LEARNING_RATE:-2e-4}
export NUM_STEPS=${NUM_STEPS:-10000}  
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export NCCL_DEBUG=INFO

torchrun \
  --nnodes=$NNODES \
  --nproc-per-node=$NPROC_PER_NODE \
  --node-rank=$RANK \
  --master-addr=$MASTER_ADDR \
  --master-port=$MASTER_PORT \
  /app/test.py \
  --nodes $NNODES \
  --gpus-per-node $NPROC_PER_NODE \
  --batch-size $BATCH_SIZE \
  --micro-batches $MICRO_BATCHES \
  --learning-rate $LEARNING_RATE \
  --num-steps $NUM_STEPS



    def _pre_post_initial_receives(self):
        """
        Pre-post initial receives for better communication-computation overlap.
        
        PURPOSE: This function implements a crucial optimization technique called 
        "pre-posting receives" that dramatically improves pipeline efficiency by 
        overlapping communication with computation.
        
        WHY IT MATTERS: Without this, each pipeline stage would wait idle during 
        communication, leading to poor GPU utilization and slower training.        
        
        WITHOUT PRE-POSTING (BAD - lots of idle time):
        Time →  0    1    2    3    4    5    6    7    8
        Stage 0: F0 |    | F1 |    | F2 |    | F3 |    |
        Stage 1:    |wait| F0 |    | F1 |    | F2 |    |  
        Stage 2:    |    |    |wait| F0 |    | F1 |    |
        Stage 3:    |    |    |    |wait| F0 |    | F1 |

        WITH PRE-POSTING (GOOD - overlapped communication):
        Time →  0    1    2    3    4    5    6    7    8  
        Stage 0: F0  F1  F2  F3  B0  B1  B2  B3           
        Stage 1: F0  F1  F2  F3  B0  B1  B2  B3           
        Stage 2: F0  F1  F2  F3  B0  B1  B2  B3           
        Stage 3: F0  F1  F2  F3  B0  B1  B2  B3           

        Key: F=Forward, B=Backward, wait=idle time due to communication      

        MEMORY COST:
        - Each pre-posted receive allocates a tensor buffer
        - max_prepost=4 means we allocate 4 extra activation tensors per stage
        - For a large model (e.g., 13B params), this might be 4 * batch_size * seq_len * hidden_dim
        - Example: 4 * 8 * 2048 * 4096 * 2 bytes = ~512 MB per stage

        PERFORMANCE BENEFIT:  
        - Eliminates communication bottlenecks
        - Can improve training throughput by 20-40%
        - Enables true pipeline parallelism instead of sequential processing       

        training loop:
        1. forward_step(micro_batch_id=0):
        - Checks: "Is step 0 in pending_forward_ops?"
        - If yes: recv_req.wait() → data is ready instantly!
        - If no: fallback to synchronous receive (slower)

        2. backward_step(micro_batch_id=0):  
        - Checks: "Is step 0 in pending_backward_ops?"
        - If yes: recv_req.wait() → gradients ready instantly!
        - If no: fallback to synchronous receive (slower)

        3. As each step completes, new receives are pre-posted for future steps
        - This maintains the "sliding window" of pre-posted operations        
        """
        logger.info(f"Rank {self.rank}: Pre-post initial receives for better communication-computation overlap started...")
        # Limit the number of pre-posted receives to avoid excessive memory usage
        # pre-post a few micro-batches ahead, not all of them        
        max_prepost = min(4, self.micro_batch_size) # Limit memory usage

        # Loop through the first few micro-batch steps
        for step in range(max_prepost):

            # Pre-post forward receives
            # Only non-first stages need to receive forward activations
            # If not the first pipeline stage (rank 0)
            if self.rank > 0:
                # Pre-post an async receive operation for forward pass data
                # This says: "Hey NCCL, I'm going to need data from the previous 
                # stage for micro-batch 'step', so start listening for it NOW"                
                buf = self.fwd_recv_pool[step % self.pool]
                recv_req, recv_tensor = self.comm_handler.async_recv_forward(buf, step)
                if recv_req: # If the receive was successfully posted
                # Store the request handle and buffer for later completion
                # This allows us to check later: "Has my data arrived yet?"
                    self.pending_forward_ops[step] = (recv_req, recv_tensor)
                # WHAT THIS ACHIEVES:
                # - Stage 1 can start receiving data for micro-batch 0 immediately
                # - Stage 2 can start receiving data for micro-batch 0 immediately  
                # - Stage 3 can start receiving data for micro-batch 0 immediately
                # - All BEFORE any actual computation starts!


            # Pre-post backward receives
            # Only non-last stages need to receive backward gradients
            # If not the last pipeline stage
            if self.rank < self.world_size - 1:
                # Pre-post an async receive for backward pass gradients
                # This says: "I'll eventually need gradients from the next stage
                # for micro-batch 'step', so start listening for them NOW"                
                buf = self.bwd_recv_pool[step % self.pool]
                recv_req, recv_grad = self.comm_handler.async_recv_backward(buf, step)
                if recv_req: # If the receive was successfully posted
                # Store for later completion during actual backward pass
                    self.pending_backward_ops[step] = (recv_req, recv_grad)
                # WHAT THIS ACHIEVES:
                # - All intermediate stages are ready to receive gradients
                # - No waiting during the actual backward pass
                # - Communication can happen while other stages compute
        
        logger.info(f"Rank {self.rank}: Pre-post initial receives for better communication-computation overlap ended...")  